{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On matrix factorizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that linear operators, and more generally, linear maps can be uniquely associated with their matrices, once a particular basis is fixed.  In fact, many linear algebra textbooks take an entirely matrix-based world view, and with a good reason.  Indeed, matrices provide a simple and systematic way of representing operators in computations, which is appreciated by mathematicians, numerical software developers, and users of linear algebra - that is pretty much by everyone.\n",
    "In this note we provide a very brief discussion of the most important matrix factorizations, give references to the standard routines for their computation in Python, and finally some matrix-interpretations of some of the theorems about operators we have proved in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular value decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular value decomposition states that for an arbitrary linear map $T\\in\\mathcal{L}(\\mathcal{V},\\mathcal{U})$ we can find two orthonormal bases in $\\mathcal{V}$ and $\\mathcal{U}$ such that the matrix of the operator with respect to these bases is diagonal with non-negative entries.  In matrix-language, we have a representation $A=U\\Sigma V^*$, where If $V^*=\\bar{V}^T$, that is, conjugated and transposed.  In this representation, $\\Sigma$ is a diagonal matrix with singular values on the main diagonal, and $U$ and $V$ are the matrices with orthonormal columns (i.e., $U^* U = I$ and $V^* V = I$) giving us left and right singular vectors.\n",
    "Let us consider an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1., 2., 3.],[4., 5., 6.]])\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute SVD of A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.3863177 , -0.92236578],\n",
       "        [-0.92236578,  0.3863177 ]]),\n",
       " array([9.508032  , 0.77286964]),\n",
       " array([[-0.42866713, -0.56630692, -0.7039467 ],\n",
       "        [ 0.80596391,  0.11238241, -0.58119908],\n",
       "        [ 0.40824829, -0.81649658,  0.40824829]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u,sigma,vh=np.linalg.svd(A)\n",
    "u,sigma,vh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\sigma$ is returned as just a list of singular values.  Let us check the orthonormality of vectors in $u$ and $v$ (note: the routine returns $v^*$ and not $v$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+00, 5.33877946e-19],\n",
       "        [5.33877946e-19, 1.00000000e+00]]),\n",
       " array([[1.00000000e+00, 5.55111512e-17, 5.55111512e-17],\n",
       "        [5.55111512e-17, 1.00000000e+00, 3.88578059e-16],\n",
       "        [5.55111512e-17, 3.88578059e-16, 1.00000000e+00]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.conjugate(u.T) @ u, vh @ np.conjugate(vh.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the appearence of small numbers in off-diagonal entries, reminding us that we do not compute in exact arithmetics.\n",
    "Let us now compute $U\\Sigma V^*$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u @ np.diag(sigma) @ vh[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we get back $A$, however we need to select only two (first) singular vectors from $v$ out of the three basis vectors there (one can simply check the dimensions of the matrices to see this).  To avoid this, one can instead compute a \"compact\" version of SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.3863177 , -0.92236578],\n",
       "        [-0.92236578,  0.3863177 ]]),\n",
       " array([9.508032  , 0.77286964]),\n",
       " array([[-0.42866713, -0.56630692, -0.7039467 ],\n",
       "        [ 0.80596391,  0.11238241, -0.58119908]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u1,sigma1,vh1=np.linalg.svd(A, full_matrices=False)\n",
    "u1,sigma1,vh1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u1 @ np.diag(sigma1) @ vh1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD is expensive to compute, but is used in many contexts: least square and inverse problems (pseudoinverse), computing approximations to operators, statistics (principal component analysis), computing ranks of matrices, bases of range and null-spaces, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Gram-Schimdt process can be used to transform any list $(a_1,\\dots,a_m)$ of linearly independent vectors into an orthonormal list $(q_1,\\dots,q_m)$ with the property that $\\text{span}(v_1,\\dots,v_j)=\\text{span}(q_1,\\dots,q_j)$, $1\\leq j \\leq m$.\n",
    "In matrix language, any matrix $A$ containing the column vectors $(a_1,\\dots,a_m)$ can be expressed as $A=QR$, where the columns of $Q$ are orthonormal (i.e., $Q^* Q = I$), which corresponds to the orthonormality of the basis  $(q_1,\\dots,q_m)$ encoded in the columns of $Q$, and $R$ is upper triangular, which represents the the equality between spans of $a$'s and $q$'s.\n",
    "Note: $QR$ factorization is normally not implemented using the textbook Gramm-Schmidt process, which does not perform very well in inexact arithmetics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.],\n",
       "       [5., 6.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1.,2.],[3.,4.],[5.,6.]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.16903085,  0.89708523],\n",
       "        [-0.50709255,  0.27602622],\n",
       "        [-0.84515425, -0.34503278]]),\n",
       " array([[-5.91607978, -7.43735744],\n",
       "        [ 0.        ,  0.82807867]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q,R=np.linalg.qr(A)\n",
    "Q,R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verufy that $Q^*Q = I$ (in the absense of round off errors) and $QR = A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.0000000e+00, 1.0875945e-16],\n",
       "        [1.0875945e-16, 1.0000000e+00]]),\n",
       " array([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.conjugate(Q.T)@Q, Q@R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QR factorization is primarily used for solving least-squares problems.  Another application of QR factorization is in eigenvalue (and singular value) computation algorithms, see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cholesky factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cholesky factorization is a representation of Hermitian positive definite matrices (which correspond to positive and invertible operators) as $A=R^*R$, where $R$ is an upper triangular matrix.  Cholesky factorization is primarily utilized for solving systems of linear algebraic equations.  After the expensive part (factorization) is computed, the solution to the original linear system $Ax=b$ is computed by solving two (simple) systems with triangular matrices (so called backward-forward substitution): $R^*y = b$ and subsequently $Rx = y$.\n",
    "Note: the routine below returns a lower triangular matrix $L=R^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.53703353, 1.51194951, 1.55626314, 1.96756012, 2.43318558],\n",
       "        [1.51194951, 1.38006151, 0.90802796, 1.25098167, 1.35826107],\n",
       "        [1.55626314, 0.90802796, 1.7131923 , 1.18795462, 1.66792259],\n",
       "        [1.96756012, 1.25098167, 1.18795462, 1.82614035, 1.86078965],\n",
       "        [2.43318558, 1.35826107, 1.66792259, 1.86078965, 2.77504445]]),\n",
       " array([[ 1.59280681,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.94923596,  0.69210736,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.97705706, -0.02807331,  0.87049623,  0.        ,  0.        ],\n",
       "        [ 1.23527857,  0.11329286, -0.01815346,  0.53578203,  0.        ],\n",
       "        [ 1.52760873, -0.13263848,  0.19717271, -0.01423398,  0.6203091 ]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# let us generate a random positive definite matrix\n",
    "n = 5\n",
    "Atmp = np.random.rand(n,n)\n",
    "A = np.conjugate(Atmp.T)@Atmp + 1.0E-10*np.eye(n)\n",
    "# we now compute a Cholesky factorization of A\n",
    "L = np.linalg.cholesky(A)\n",
    "A, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us not compute the norm of the difference between $A$ and $LL^*$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.364386412590295e-16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A-L@np.conjugate(L.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is a very small number, same order of magnitude as the round off error.\n",
    "\n",
    "In the context of solving linear systems, we proceed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6504651808933464e-15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.rand(n)\n",
    "y = np.linalg.solve(L,b)\n",
    "x = np.linalg.solve(np.conjugate(L).T,y)\n",
    "np.linalg.norm( x - np.linalg.solve(A,b) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So indeed, the difference between the solutions to $Ax=b$ and those to $Ly=b$, $L^*x = y$ is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LU-factorization is a representation of a square matrix as a product of two triangular matrices: a lower triangular matrix $L$, and an upper triangular matrix $U$.  LU factorization is equivalent to a matrix-representation of Gaussian elimination.  Its computation costs approximately twice as much as the computation of Cholesky factorization, but the factorized matrix does not have to be symmetric (real case)/Hermitian (complex case), or positive definite.  Assuming that the representation $A=LU$ exists, one can compute the elements of $L$ and $U$ one by one, starting from the top left corner of $A$.  There is ambiguity in the computation of diagonal elements of $L$ and $U$, which is typically removed by assuming that $L_{ii}=1$.\n",
    "Unfortunately, not all invertible matrices admit an $LU$ factorization.  To deal with this issue, we allow the rows of the matrix $A$ to be permuted, which results in a factorization $A=PLU$, where $P$ is just a permutation matrix.\n",
    "Such decompositions exist for arbitrary invertible matrices, and are called LU factoriszation with pivoting.  Once the factorization is computed, one solves the system $Ax=b$ as $Ly = P^T b$, $Ux = y$, similarly to the case of Cholesky:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7763568394002505e-15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "n = 5\n",
    "A = np.tril(np.reshape(np.arange(n*n)+1,(n,n)))\n",
    "P,L,U = sl.lu(A)\n",
    "np.linalg.norm(A-P@L@U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now solve a linear system and check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.rand(n)\n",
    "y = np.linalg.solve(L,P.T@b)\n",
    "x = np.linalg.solve(U,y)\n",
    "np.linalg.norm(x - np.linalg.solve(A,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalue decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that normal operators (complex case) and aelf-adjoint operators (real case) can be diagonalized using an orthonormal basis of eigenvectors.  In the matrix language, for example in the real case we can represent any $A$ as a product $VDV^T$ with $D$ being a diagonal matrix containing the eigenvalues on the diagonal, and columns $V$ representing the orthonormal eigevectors, i.e., $V^*V = I$.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.58447642e-04, 4.74927664e-02, 3.26943232e-01, 7.39921632e-01,\n",
       "        9.99674337e+00]),\n",
       " array([[ 0.47226796, -0.64334721,  0.19717323,  0.37628869, -0.42731357],\n",
       "        [-0.61654676,  0.1528263 ,  0.58255302,  0.2013031 , -0.46542803],\n",
       "        [-0.36944232, -0.2334876 , -0.69716683, -0.18169019, -0.53846355],\n",
       "        [ 0.35087338,  0.68679412, -0.25574958,  0.45715356, -0.36166902],\n",
       "        [ 0.37045015,  0.19116974,  0.26515137, -0.75886967, -0.42430225]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "n = 5\n",
    "Atmp = np.random.rand(n,n)\n",
    "A = Atmp.T @ Atmp\n",
    "d,V = sl.eigh(A)\n",
    "d,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9856549075576475e-14, 7.322422964396005e-16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(V @ np.diag(d) @ V.T - A), np.linalg.norm(V.T@V - np.eye(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course one can also request to compute the eigenvalues/vectors of non-normal operators, however in this case we cannot expect the eigenvectors to produce a basis of the space (or to be orthonormal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "n = 3\n",
    "N = np.diag(np.ones(n),1)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]),\n",
       " array([[ 1.00000000e+000, -1.00000000e+000,  1.00000000e+000,\n",
       "         -1.00000000e+000],\n",
       "        [ 0.00000000e+000,  4.00833672e-292, -4.00833672e-292,\n",
       "          4.00833672e-292],\n",
       "        [ 0.00000000e+000,  0.00000000e+000,  0.00000000e+000,\n",
       "         -0.00000000e+000],\n",
       "        [ 0.00000000e+000,  0.00000000e+000,  0.00000000e+000,\n",
       "          0.00000000e+000]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d,V = sl.eig(N)\n",
    "d, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this nilpotent matrix has only one eigenvalue zero of multiplicity equal to the size of the matrix. There is only one (non-generalized) eigenvector, and columns of $V$ are all the same (hence also lineraly dependent) and do not constitute the basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jordan form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jordan form computation is numerically unstable, because it relies on identifying whether the computed eigenvalues are exactly identical (rarely happens in floating point arithmetics) or not.  For small matrices, one can perform a symbolic computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 & 1 & 0 & 0\\\\0 & 2 & 0 & 0\\\\0 & 0 & 2 & 1\\\\0 & 0 & 0 & 2\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[2, 1, 0, 0],\n",
       "[0, 2, 0, 0],\n",
       "[0, 0, 2, 1],\n",
       "[0, 0, 0, 2]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy.matrices import Matrix\n",
    "A = Matrix([[ 6,  5, -2, -3], [-3, -1,  3,  3], [ 2,  1, -2, -3], [-1,  1,  5,  5]])\n",
    "P, J = A.jordan_form()\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try the same using floating point arithmetics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.00000005+0.j, 1.99999995+0.j, 1.99999996+0.j, 2.00000004+0.j]),\n",
       " array([[ 0.73029675, -0.73029674,  0.60135886, -0.60135888],\n",
       "        [-0.54772255,  0.54772256, -0.52944886,  0.52944886],\n",
       "        [ 0.36514837, -0.36514837,  0.45753886, -0.45753885],\n",
       "        [-0.18257418,  0.18257419, -0.38562885,  0.38562884]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "A_numpy = np.array(A,dtype=np.float64)\n",
    "d,V = sl.eig(A_numpy)\n",
    "d, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that all the computed eigenvalues are (slightly) different, and the corresponding eigenvectors are therefore linearly independent (or only almost linearly dependent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schur decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in a complex vector space for an arbitrary operator there is a basis, in which the operator's matrix is upper triangular.  By applying Gram-Schmidt orthogonalization to such a basis, we conclude that the same is true with an additional assertion that the basis can be selected to be orthonormal.  This theorem is due to Schur, and the corresponding matrix factorization $A = Q R Q^*$ is known as Schur factorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5.22017787e+00-5.13568015e-16j, -2.33374845e-01+1.93320337e-01j,\n",
       "         -5.94006001e-01-1.51015397e-01j,  3.71176558e-01+7.68488118e-02j,\n",
       "         -3.34602935e-02+8.28947292e-02j, -7.24324475e-03-6.43658449e-02j,\n",
       "         -1.45030762e-01+4.13250948e-01j,  3.91048974e-01-1.17596487e-01j,\n",
       "          6.41138617e-02+3.98298525e-02j,  1.53182670e-01+3.13310727e-02j],\n",
       "        [ 0.00000000e+00+0.00000000e+00j, -1.32147170e+00+1.39676539e-17j,\n",
       "          1.49723289e-01+9.56416130e-02j,  2.13110954e-02-1.40626769e-01j,\n",
       "         -6.32784780e-02+1.14879246e-01j, -9.47718921e-03+1.93328518e-01j,\n",
       "         -1.19013611e-01+4.41356561e-02j,  1.12456196e-01+2.01226885e-02j,\n",
       "         -1.80331511e-01-5.38558034e-01j,  4.25303776e-02+5.28910330e-02j],\n",
       "        [ 0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          5.48615829e-01+5.75613290e-01j, -2.64044138e-02+5.76510684e-01j,\n",
       "          2.82320868e-01+6.38298357e-02j,  6.36650166e-02+1.02491323e-01j,\n",
       "         -2.79920022e-01+3.52543904e-02j, -1.87976603e-01-3.02176081e-01j,\n",
       "         -5.70354073e-02-2.74544236e-01j, -1.70578247e-01-5.74621156e-02j],\n",
       "        [ 0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  5.48615829e-01-5.75613290e-01j,\n",
       "          3.46609422e-01-4.57266778e-02j, -1.90355143e-01-1.11435874e-01j,\n",
       "          2.40706180e-01-1.01460064e-02j,  7.77835970e-02+1.36639545e-01j,\n",
       "          1.95107737e-01-3.49490961e-01j,  9.75097205e-02-1.06060080e-01j],\n",
       "        [ 0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          3.74878902e-01+9.46480567e-02j, -2.59962277e-01+2.11108703e-01j,\n",
       "          2.03280366e-02-2.09871371e-01j,  7.59711819e-02-1.88411486e-01j,\n",
       "          9.12545904e-02-2.71290282e-01j,  1.08924408e-02-1.11527554e-01j],\n",
       "        [ 0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  3.74878902e-01-9.46480567e-02j,\n",
       "         -1.88789093e-01+8.92948743e-02j, -2.13901023e-01+1.57953851e-01j,\n",
       "          1.35446210e-02+7.95540224e-02j,  4.96048142e-02-1.47527376e-02j],\n",
       "        [ 0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "         -1.62456285e-01+4.25087531e-01j,  3.78244722e-03+1.06354489e-01j,\n",
       "          8.47703587e-02+8.48359265e-02j,  3.05799528e-01+6.63369601e-02j],\n",
       "        [ 0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j, -1.62456285e-01-4.25087531e-01j,\n",
       "          5.54731461e-02-1.01087388e-01j,  1.15241808e-01-2.96078745e-01j],\n",
       "        [ 0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "         -4.21197669e-01-1.06511000e-16j, -2.74366352e-01+1.01439718e-01j],\n",
       "        [ 0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
       "          0.00000000e+00+0.00000000e+00j, -3.43015547e-01+1.31863492e-16j]]),\n",
       " array([[ 0.30184122+0.0409343j , -0.28602458+0.17813302j,\n",
       "         -0.19128031-0.16808643j,  0.19265535-0.07651668j,\n",
       "          0.02298584+0.21588653j,  0.1680189 +0.31765616j,\n",
       "         -0.24668365-0.36398448j, -0.33881796-0.36462847j,\n",
       "         -0.0727235 -0.06010466j,  0.23043817+0.08061963j],\n",
       "        [ 0.37945394+0.05145978j,  0.29444483-0.18337707j,\n",
       "         -0.3051956 -0.23209173j,  0.27864894-0.07483121j,\n",
       "         -0.04017592+0.04187599j, -0.01747304-0.09038515j,\n",
       "          0.31365129+0.07213118j,  0.04415048+0.33194564j,\n",
       "         -0.35092075-0.29002967j,  0.24607339+0.08608967j],\n",
       "        [ 0.33781044+0.04581228j,  0.31931324-0.19886484j,\n",
       "          0.41882419+0.21650599j, -0.30118379-0.03083093j,\n",
       "         -0.15790572+0.30263895j,  0.02227995-0.20217228j,\n",
       "         -0.19587315-0.11312646j, -0.09495251-0.23024379j,\n",
       "         -0.27077997-0.22379476j, -0.17362108-0.06074197j],\n",
       "        [ 0.33949858+0.04604122j,  0.17554643-0.10932842j,\n",
       "         -0.16758021+0.30685408j, -0.19278196+0.52744001j,\n",
       "          0.2185788 -0.49144808j, -0.07862341+0.19943802j,\n",
       "          0.04073639-0.16622522j, -0.16805339-0.01606937j,\n",
       "          0.06895813+0.05699265j,  0.03272986+0.01145066j],\n",
       "        [ 0.24964218+0.03385531j, -0.18537425+0.11544908j,\n",
       "         -0.42753655+0.03893347j,  0.10048158+0.37176119j,\n",
       "         -0.11028208+0.22384261j,  0.02378167-0.12736205j,\n",
       "         -0.2474565 +0.3325502j ,  0.35062027-0.13062734j,\n",
       "         -0.03331427-0.02753364j, -0.38165562-0.1335236j ],\n",
       "        [ 0.23919624+0.03243868j, -0.20266941+0.12622032j,\n",
       "          0.12979006-0.19083246j,  0.11202688-0.34720222j,\n",
       "          0.12335559-0.61976552j, -0.26997192-0.26712234j,\n",
       "         -0.19791411+0.04406932j,  0.06080382-0.17926453j,\n",
       "         -0.16040037-0.13256802j, -0.17545378-0.06138314j],\n",
       "        [ 0.34280001+0.04648894j,  0.37951069-0.23635516j,\n",
       "          0.02866542-0.21352923j,  0.16119702-0.30103741j,\n",
       "          0.05636175+0.02806262j,  0.08170687+0.22305462j,\n",
       "         -0.09484422+0.09521797j,  0.10247538-0.06093272j,\n",
       "          0.47806259+0.39511011j, -0.18898102-0.06611569j],\n",
       "        [ 0.31418785+0.04260869j, -0.20138458+0.12542014j,\n",
       "          0.00408277+0.0470896j , -0.03874843+0.05858107j,\n",
       "         -0.22068976+0.12044899j, -0.16817733-0.61799673j,\n",
       "          0.1532356 -0.14498283j, -0.1567994 +0.10143131j,\n",
       "          0.36222477+0.29937224j,  0.23577837+0.08248792j],\n",
       "        [ 0.3426386 +0.04646705j, -0.36905893+0.22984592j,\n",
       "          0.24574574+0.06267567j, -0.12547682-0.10234251j,\n",
       "          0.05846754+0.08792621j,  0.12350998+0.29660354j,\n",
       "          0.402653  -0.01786002j, -0.05264449+0.38890954j,\n",
       "         -0.03066574-0.02534468j, -0.38738761-0.13552896j],\n",
       "        [ 0.25359201+0.03439097j, -0.15171694+0.09448767j,\n",
       "          0.27338651+0.13542909j, -0.19190365-0.02784149j,\n",
       "          0.06092741-0.00657551j,  0.06400655+0.20019549j,\n",
       "         -0.14471911+0.40464394j,  0.41305039-0.00556235j,\n",
       "          0.01054814+0.00871784j,  0.5729407 +0.20044538j]]),\n",
       " 5.067332092009266e-15)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "n = 10\n",
    "A = np.random.rand(n,n)\n",
    "R, Q = sl.schur(A,output='complex')\n",
    "R, Q, np.linalg.norm(np.conjugate(Q.T)@Q-np.eye(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $A$ and $R$ are matrices correspoinding to the same operator, just in different bases. The eigenvalues of a triangular matrix are on the diagonal, so computation of the Schur form is equivalent to computing all the eigenvalues of the matrix (and is therefore quite expensive).  In fact, one of the popular eigenvalue computation algorithms, QR-algorithm, iteratively transforms a matrix into a Schur form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR algorithm for computing eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have previously considered power iteration algorithm for computing the eigenvalue of a matrix with the largest magnitude.  One can extend this idea for computing all eigenvalues simultaneously by considering the following iteration: $A_0=A$, $Q_k R_k = A_k$, $A_{k+1} = R_k Q_k$, $k=0,1,\\dots$\n",
    "One can expect, under certain conditions, that $A_k$ converges to an upper triangular matrix. In fact, \n",
    "$A^{k+1} = Q_0 R_0 Q_0 R_0 \\dots Q_0 R_0 = Q_0 A_1 \\dots A_1 R_0 = Q_0 Q_1 R_1 \\dots Q_1 R_1 R_0 = \\dots$.  Therefore, $Q = Q_0 Q_1 \\dots Q_k$ and $R = R_k R_{k-1} \\dots R_0$ constitute the QR factorization of the \"power iteration\" matrix $A^{k+1}$.\n",
    "Omitting several important details, the simplest implenentation of such an eigenvalue computation algorithm is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of A, scipy:\n",
      "[1.86730047e-07 8.05589058e-02 3.32879116e-01 1.49961724e+00\n",
      " 6.49927971e+00]\n",
      "Approximate eigenvalues of A, QR-algorithm\n",
      "[1.86730047e-07 8.05589058e-02 3.32879116e-01 1.49961724e+00\n",
      " 6.49927971e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "n = 5\n",
    "Atmp = np.random.rand(n,n)\n",
    "A = Atmp.T @ Atmp\n",
    "d,V = sl.eigh(A)\n",
    "print(\"Eigenvalues of A, scipy:\")\n",
    "print(d)\n",
    "\n",
    "epsilon = 1.0E-08\n",
    "while np.linalg.norm(np.tril(A,-1)) > epsilon*np.linalg.norm(A):\n",
    "    Q,R = np.linalg.qr(A)\n",
    "    A   = R@Q\n",
    "print(\"Approximate eigenvalues of A, QR-algorithm\")\n",
    "print(np.sort(np.diag(A)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since singular values of an operator are simply the eigenvalues of a related self-adjoint operator, the same algorithm can be used for computing singular values as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some concluding remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all but the tiniest matrices $A$, avoid computing $\\det(A)$ and $A^{-1}$, as these computations are both expensive and get inaccurate very fast because of round-off errors. Instead of computing $A^{-1}$, factorize the matrix: use Cholesky, whenever possible (for positive definite Hermitian matrices), LU with pivoting (permutations), when not.  These factorizations allow one to quickly solve systems of linear algebraic equations, that is,  apply the operator associated with the matrix $A^{-1}$.  For particularly difficult systems of linear algebraic equations, or least squares problems, QR factorization could be more appropriate.  Computing all singular or eigenvalues is very expensive, and is practically impossible for all but relatively small problems.  In fact, even computing Cholesky, LU, or QR factorization becomes prohibitevly expensive for large problems - and special iterative algorithms are instead utilized for solving systems of linear algebraic equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
